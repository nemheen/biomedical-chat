{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AWac_MXtT8Gp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Specs Detail"
      ],
      "metadata": {
        "id": "UFGdd26wYB1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "import torch\n",
        "\n",
        "# CPU Model\n",
        "cpu_info = !lscpu | grep \"Model name\"\n",
        "print(f\"CPU: {cpu_info[0]}\")\n",
        "\n",
        "# Number of CPU Cores\n",
        "# physical_cores = psutil.cpu_count(logical=False)\n",
        "# logical_cores = psutil.cpu_count(logical=True)\n",
        "# print(f\"CPU Cores: {physical_cores} physical, {logical_cores} logical\")\n",
        "\n",
        "# Total RAM\n",
        "ram = psutil.virtual_memory()\n",
        "print(f\"Total RAM: {ram.total / 1e9:.2f} GB\")\n",
        "\n",
        "# Disk Space\n",
        "disk = psutil.disk_usage('/')\n",
        "print(f\"Disk Space: {disk.total / 1e9:.2f} GB\")\n",
        "\n",
        "# GPU Details\n",
        "gpu_name = !nvidia-smi --query-gpu=name --format=csv,noheader\n",
        "if torch.cuda.is_available():\n",
        "    device_id = torch.cuda.current_device()  # Get current CUDA device index\n",
        "    print(f\"CUDA Device ID: {device_id}\")\n",
        "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")\n",
        "\n",
        "print(pyth)"
      ],
      "metadata": {
        "id": "neCrfIMVO-Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "vlx6-WtAXMP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drive connect"
      ],
      "metadata": {
        "id": "dQZBcmE_73WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Datasets and Basic Analyzing\n"
      ],
      "metadata": {
        "id": "BSri_e7wYKCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdnVUPLtOoaV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/datasets'\n"
      ],
      "metadata": {
        "id": "zHQhtbV_VV4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "TTSUXalXVgFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset #1"
      ],
      "metadata": {
        "id": "EWdU0z4xT2Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "d1_path = \"/content/drive/MyDrive/datasets/advs7235-sup-0001-suppmat.csv\"\n",
        "\n",
        "d1 = pd.read_csv(d1_path)\n",
        "d1.head()"
      ],
      "metadata": {
        "id": "NtScOmbJQywP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1.describe()"
      ],
      "metadata": {
        "id": "WYpjzeSOSSKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1.isnull().sum()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RVVzkw1hwYIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests pandas tools\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uusEuY1YzIpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "!pip install --upgrade pymupdf\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tKaj0ZbUYfTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdf2image pytesseract\n"
      ],
      "metadata": {
        "id": "75OXPWprcZcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "print(dir(fitz))"
      ],
      "metadata": {
        "id": "crvvic6aZ9IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(d1))\n",
        "print(len(d1.columns))\n",
        "print(d1.columns)"
      ],
      "metadata": {
        "id": "vLA-vpb9Sf7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch Abstract text from DOIs using Unpaywall or Crossref API to get open-access full text or abstract."
      ],
      "metadata": {
        "id": "sOUTWyBdtRtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from time import sleep\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "patterns = [\n",
        "    r'https?://\\S+',    # Remove URLs\n",
        "    r'\\bdoi\\b[\\w\\./:]+', # Remove DOI patterns\n",
        "    r'\\b(Copyright|Ltd|Company|All Rights Reserved|Trademark)\\b.*', # Remove copyright, Ltd, etc.\n",
        "    r'[^\\w\\s]', # Remove non-alphanumeric symbols\n",
        "    r\"\\s+\", # whitespace\n",
        "]\n",
        "\n",
        "\n",
        "def crop_text(text):\n",
        "    # Find the first occurrence of 'Introduction' and the last occurrence of 'References', 'Acknowledgements', etc.\n",
        "    start_index = re.search(r'\\bIntroduction\\b', text)\n",
        "    end_index = re.search(r'\\b(?:References|Acknowledgements|Conflicts of Interest)\\b', text)\n",
        "\n",
        "    # Ensure the keywords exist and crop the text\n",
        "    if start_index and end_index:\n",
        "        text = text[start_index.start():end_index.end()]\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove HTML tags (for Crossref abstract)\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = crop_text(text)\n",
        "\n",
        "    # Remove URLs\n",
        "    for pattern in patterns:\n",
        "      text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Remove page numbers, headers/footers (common artifacts)\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "\n",
        "        if re.match(r\"^Page\\s*\\d+$\", line) or len(line) < 5:\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    return \" \".join(cleaned_lines)\n",
        "\n",
        "\n",
        "def get_open_access_url(doi):\n",
        "    url = f\"https://api.unpaywall.org/v2/{doi}?email=sharon.nemekhbayar1009@gmail.com\"\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        data = res.json()\n",
        "        return data.get(\"best_oa_location\", {}).get(\"url_for_pdf\") or \\\n",
        "               data.get(\"best_oa_location\", {}).get(\"url\")\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_crossref_abstract(doi):\n",
        "    url = f\"https://api.crossref.org/works/{doi}\"\n",
        "    try:\n",
        "        res = requests.get(url)\n",
        "        data = res.json()\n",
        "        return data[\"message\"].get(\"abstract\")\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_pdf_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
        "                text = \"\"\n",
        "                is_scanned_pdf = False\n",
        "                for page in doc:\n",
        "                    page_text = page.get_text()\n",
        "                    if not page_text.strip():\n",
        "                        is_scanned_pdf = True\n",
        "                    text += page_text\n",
        "\n",
        "                if text.strip() == \"\" and is_scanned_pdf:\n",
        "                    # If no text is found and it's a scanned PDF, use OCR to extract text\n",
        "                    images = convert_from_path(url)\n",
        "                    text = \"\"\n",
        "                    for image in images:\n",
        "                        text += pytesseract.image_to_string(image)\n",
        "                return text\n",
        "    except Exception as e:\n",
        "        print(\"PDF extraction failed:\", e)\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "df = d1.copy()\n",
        "full = 0\n",
        "abs = 0\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    doi = str(row[\"DOI\"]).strip()\n",
        "    if doi == \"nan\" or not doi or doi in processed_dois:\n",
        "        continue\n",
        "\n",
        "    # Unpaywall for full text\n",
        "    full_text_url = get_open_access_url(doi)\n",
        "    if full_text_url:\n",
        "        pdf_text = extract_text_from_pdf_url(full_text_url)\n",
        "        if pdf_text:\n",
        "            cleaned = clean_text(pdf_text)\n",
        "            docs.append({\"text\": cleaned, \"metadata\": row.to_dict()})\n",
        "            print(f\"[{i}] Full text extracted and cleaned.\")\n",
        "            full += 1\n",
        "            continue\n",
        "\n",
        "    abstract = get_crossref_abstract(doi)\n",
        "    if abstract:\n",
        "        cleaned = clean_text(abstract)\n",
        "        docs.append({\"text\": cleaned, \"metadata\": row.to_dict()})\n",
        "        print(f\"[{i}] Abstract fetched and cleaned.\")\n",
        "        abs += 1\n",
        "    else:\n",
        "        print(f\"[{i}] Nothing found for DOI: {doi}\")\n",
        "\n",
        "    sleep(1)\n",
        "\n",
        "print(f\"âœ… Full texts added: {full}\")\n",
        "print(f\"ðŸ“„ Abstracts added: {abs}\")\n",
        "print(f\"ðŸ“š Total new documents added: {full + abs}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WiAVlw5jzLqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)\n"
      ],
      "metadata": {
        "id": "JctTXgrGksYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_dois = {entry[\"metadata\"][\"DOI\"].strip().lower() for entry in docs if \"DOI\" in entry[\"metadata\"]}\n",
        "processed_dois"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MWooPyrLgcfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the fetched texts"
      ],
      "metadata": {
        "id": "zW6Ueshn8hVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "documents = json.load(open(\"documents.json\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "8mZdBSzC9SVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "\n",
        "def normalize_unicode_ligatures(text):\n",
        "    return unicodedata.normalize(\"NFKD\", text)\n",
        "\n",
        "patterns = [\n",
        "    r'\\bdoi\\b[\\w\\./:]+',  # DOI patterns\n",
        "    r'\\(\\s*(et al\\.,?|[A-Z][a-z]+[^)]*\\d{4})\\s*\\)',  # Citations like (Smith et al., 2021)\n",
        "    r'^\\s*Figure\\s*\\d+[a-zA-Z]?[.:]?\\s*(shows|illustrates|depicts|demonstrates|explains|describes|represents)\\s*[:]*\\s*',\n",
        "    r'\\[\\s*\\d{1,3}(?:[\\s,â€“-]*\\d{1,3})*\\s*\\]',  # citation references like [24â€“29,46,53,54]\n",
        "    r'\\[.*?\\.(avi|mp4|pdf|docx|zip|pptx|xls|xlsx|txt)\\]',  # file references\n",
        "    r'\\b([A-Za-z\\s]+\\.?\\s?\\d{4},\\s?\\d+,\\s?[\\d\\-â€“,]+(?:\\.\\s?\\d+)?)\\b',  # journal references\n",
        "    r'https?://\\S+',  # Remove full URLs\n",
        "    r'\\b(?:www|https?)\\.[\\w\\.-]+\\.\\w+\\b',  # domain names\n",
        "    r':\\/\\/\\s*:\\/{2,}',  # malformed URL\n",
        "    r'/journal/\\s*\\d+\\s*of\\s*\\d+',  # journal path patterns\n",
        "    r'\\s+',  # Collapse whitespace\n",
        "    r'\\(?(Fig(?:ure)?\\s*\\d*[a-zA-Z]?)\\)?',        # (Fig 3.a), (Figure a), Figure b\n",
        "    r'\\(?(appendix\\s*\\d+(?:\\.\\d+)?)\\)?',          # (appendix 1.2), appendix 2.1\n",
        "    r'Figure\\s+[a-zA-Z]',                         # Figure a, Figure b\n",
        "    r'\\(Fig\\s+[a-zA-Z]\\)',                        # (Fig a), (Fig B)\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "keywords = [\n",
        "    \"Acknowledgement\", \"Acknowledgements\", \"ACKNOWLEDGMENTS\", \"Acknowledgment\", \"Acknowledgments\", \"ACKNOWLEDGEMENTS\",\n",
        "    \"Conflict of Interest\", \"Conflicts of Interest\",\n",
        "    \"Supporting Information\", \"Funding\", \"Author Contributions\",\n",
        "    \"Ethics\", \"Abbreviations\", \"Data Availability\",\n",
        "    \"ORCID\", \"Consent\", \"Supplementary Material\", \"References\", \"Reference\"\n",
        "]\n",
        "\n",
        "# Crop full text to useful section only\n",
        "def crop_text(text):\n",
        "    start = re.search(r'\\bIntroduction\\b', text, re.IGNORECASE)\n",
        "    end = re.search(r'\\b(References|Acknowledgements|Conflicts of Interest|Supporting Information|supplementary)\\b', text[::-1], re.IGNORECASE)\n",
        "\n",
        "    start_idx = start.start() if start else 0\n",
        "    end_idx = end.start() if end else len(text)\n",
        "\n",
        "    return text[start_idx:end_idx]\n",
        "\n",
        "# Main cleaner\n",
        "def clean_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove HTML if present (e.g., in Crossref abstracts)\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Crop to core content\n",
        "    text = crop_text(text)\n",
        "\n",
        "    # Apply all regex patterns\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    for k in keywords:\n",
        "        p = rf\"{k}[:\\[\\]\\-]?\\s*[\\s\\S]*?$\"\n",
        "        text = re.sub(p, \"\", text)\n",
        "\n",
        "    # Remove hyphenation artifacts (e.g., \"bio-\\nchemical\" => \"biochemical\")\n",
        "    text = re.sub(r'-\\s+', '', text)\n",
        "    # Fix hyphenated line breaks (e.g., \"den-\\nsity\" â†’ \"density\")\n",
        "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
        "    text = text.replace('\\n', ' ')\n",
        "\n",
        "    # Line-by-line cleanup\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if re.match(r'^Page\\s*\\d+(\\sof\\s*\\d+)?$', line, re.IGNORECASE):\n",
        "            continue\n",
        "        if len(line) < 5:\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "\n",
        "    text = normalize_unicode_ligatures(text)\n",
        "\n",
        "    # Final normalization\n",
        "    return re.sub(r'\\s+', ' ', ' '.join(cleaned_lines)).strip()\n"
      ],
      "metadata": {
        "id": "9zo8WmTE8lim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in nine:\n",
        "      n['text'] = clean_text(n['text'])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ooR92EnwwWW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nine, f, ensure_ascii=False, indent=2)\n"
      ],
      "metadata": {
        "id": "OUdmXZfDzqdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essential_keys = [\n",
        "    \"DOI\", \"Article Title\", \"Authors\",\n",
        "    \"Source Title\", \"Publication Year\", \"Volume\", \"Start Page\",\n",
        "    \"End Page\", \"Document Type\", \"Author Keywords\"\n",
        "]\n",
        "\n",
        "cleaned_docs_eight = []\n",
        "\n",
        "for doc in docs:\n",
        "    cleaned_text = clean_text(doc[\"text\"])\n",
        "    if len(cleaned_text) < 50:\n",
        "        continue\n",
        "    cleaned_metadata = {\n",
        "        key: doc[\"metadata\"].get(key)\n",
        "        for key in essential_keys\n",
        "        if key in doc[\"metadata\"] and pd.notna(doc[\"metadata\"][key])\n",
        "    }\n",
        "\n",
        "    cleaned_docs_eight.append({\n",
        "        \"text\": cleaned_text,\n",
        "        \"metadata\": cleaned_metadata\n",
        "    })\n"
      ],
      "metadata": {
        "id": "UQfi4C_c9eIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cleaned_docs_five is deemed to be the best of all, so saved as docs.json"
      ],
      "metadata": {
        "id": "eGCt81ljMtO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[3]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6GHIJRtl9jKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Conclusion' in documents[3]['text']"
      ],
      "metadata": {
        "id": "h06m3BGIJ8Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"docs_cleaned_eight.json\", \"w\") as f:\n",
        "    json.dump(documents, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "uIcxoDhVM2vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "T02d6VCgyrfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checks = {\n",
        "    \"dangling_references\": r'\\[\\s*\\d+([â€“,-]\\s*\\d+)*(,\\s*\\d+)*\\s*\\]',\n",
        "    \"broken_words\": r'\\b\\w{1,2}\\s+\\w{1,2}\\b',\n",
        "    \"file_links\": r'\\.(pdf|avi|mp4|zip|docx|pptx|xlsx|txt)',\n",
        "    \"journals_left\": r'/journal/|https?://',\n",
        "    \"supporting_info\": r'Supporting Information',\n",
        "    \"acknowledgments\": r'Acknowledg?ments?:',\n",
        "    \"conflicts_of_interest\": r'Conflicts of Interest',\n",
        "    \"weird_punctuation\": r'[^\\w\\s\\.,:;()\\-â€“â€”]',\n",
        "    \"page_numbers\": r'\\b\\d+\\s+of\\s+\\d+\\b',\n",
        "}\n",
        "for doc in documents:\n",
        "    text = doc[\"text\"]\n",
        "    for label, pattern in checks.items():\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            print(f\"{label} found: {matches[:3]}\")\n"
      ],
      "metadata": {
        "id": "PjvzSpms0g7A",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing evaluation"
      ],
      "metadata": {
        "id": "vFM9-upWHQr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### biorag_eval"
      ],
      "metadata": {
        "id": "L4ONeeq5KKII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rag_df = pd.read_csv('rag_eval/rag_final.csv')\n",
        "rag_df.head(3)"
      ],
      "metadata": {
        "id": "ur7vMD8mOEIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_correct = rag_df['ground_truth'] == rag_df['predicted']\n",
        "is_correct.sum()"
      ],
      "metadata": {
        "id": "2MwgSW8AOHVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_df['is_correct'] = is_correct\n",
        "rag_df.groupby('category')['is_correct'].value_counts()"
      ],
      "metadata": {
        "id": "eJpKQ0pmOWhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### biollm_eval"
      ],
      "metadata": {
        "id": "PuasFX0qKMzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "bio = pd.read_csv('rag_eval/bio.csv')\n",
        "is_correct_bio = bio['ground_truth'] == bio['answer']\n",
        "is_correct_bio.sum()"
      ],
      "metadata": {
        "id": "cQ8O0fZjY9zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio['is_correct'] = is_correct_bio\n",
        "bio.groupby('category')['is_correct'].value_counts()"
      ],
      "metadata": {
        "id": "qSHXUTx8ZETc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### basemodel_eval"
      ],
      "metadata": {
        "id": "dlHER9WTKTcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base = pd.read_csv('rag_eval/base.csv')\n",
        "is_correct_base = base['ground_truth'] == base['answer']\n",
        "\n",
        "\n",
        "base['is_correct'] = is_correct_base\n",
        "base.groupby('category')['is_correct'].value_counts()"
      ],
      "metadata": {
        "id": "x1HhjBdZZP-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_correct_base.sum()"
      ],
      "metadata": {
        "id": "4Km2AvRpzKvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "rag_df['model'] = \"BioLLM+RAG\"\n",
        "bio['model'] = \"BioLLM\"\n",
        "base['model'] = \"Base model\"\n"
      ],
      "metadata": {
        "id": "NUxlXIQdsOyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = pd.concat([rag_df, bio, base], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "NfOeGTI1OsPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate accuracy per category and model\n",
        "acc_df = (\n",
        "    all_df.groupby(['model', 'category'])['is_correct']\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"is_correct\": \"accuracy\"})\n",
        ")\n",
        "\n",
        "# Sort categories if needed (optional)\n",
        "category_order = sorted(acc_df['category'].unique())\n",
        "\n",
        "# Define custom color palette\n",
        "palette = {\n",
        "    \"Base model\": \"#4c72b0\",\n",
        "    \"BioLLM\": \"#55a868\",\n",
        "    \"BioLLM+RAG\": \"#c44e52\"\n",
        "}\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "barplot = sns.barplot(\n",
        "    data=acc_df,\n",
        "    x=\"category\",\n",
        "    y=\"accuracy\",\n",
        "    hue=\"model\",\n",
        "    palette=palette,\n",
        "    edgecolor=\"black\",\n",
        "    dodge=True,\n",
        "    errwidth=1.5,\n",
        "    width=0.7\n",
        ")\n",
        "\n",
        "\n",
        "for bar in barplot.patches:\n",
        "    height = bar.get_height()\n",
        "    barplot.annotate(\n",
        "        f'{height:.2f}',\n",
        "        (bar.get_x() + bar.get_width() / 2, height),\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=10,\n",
        "        color='black',\n",
        "        xytext=(0, 5),\n",
        "        textcoords='offset points'\n",
        "    )\n",
        "\n",
        "plt.title(\"Model Accuracy by Category\", fontsize=16, fontweight='bold')\n",
        "plt.xlabel(\"Question Category\", fontsize=13)\n",
        "plt.ylabel(\"Accuracy\", fontsize=13)\n",
        "plt.xticks(fontsize=11)\n",
        "plt.yticks(fontsize=11)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "plt.legend(title=\"Model\", title_fontsize=12, fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dTzTJSJ5satA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset #2"
      ],
      "metadata": {
        "id": "pH57nXoxT508"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2_path = \"/content/drive/MyDrive/datasets/advs7235-sup-0002-suppmat.csv\"\n",
        "\n",
        "d2 = pd.read_csv(d2_path, index_col=False)\n",
        "d2.head()"
      ],
      "metadata": {
        "id": "vY-vrn3mSCc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2.tail(150)"
      ],
      "metadata": {
        "id": "L7ncjt62vj9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2['question'][0]"
      ],
      "metadata": {
        "id": "fKnWFnk2Vun4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2['answer'][0]"
      ],
      "metadata": {
        "id": "93CodwIHV7uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(d2))\n",
        "print(len(d2.columns))\n",
        "print(d2.columns)"
      ],
      "metadata": {
        "id": "81J2lkRtTZB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2.columns\n"
      ],
      "metadata": {
        "id": "g1WSkQoKVOGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need columns ['Unnamed: 0.1', 'Unnamed: 0'], so drop them."
      ],
      "metadata": {
        "id": "IhPodZ7KeW-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2 = d2.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])"
      ],
      "metadata": {
        "id": "arLw1K52eWEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2.isnull().sum()"
      ],
      "metadata": {
        "id": "hYqN3WhDmSGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drop the rows with null"
      ],
      "metadata": {
        "id": "dFxcCqJanldQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2_cleaned = d2.dropna(subset=['question', 'answer'])\n",
        "\n",
        "# Verify the rows were dropped\n",
        "print(d2_cleaned.isnull().sum())\n",
        "\n",
        "\n",
        "print(len(d2_cleaned))\n",
        "print(len(d2_cleaned.columns))\n",
        "print(d2_cleaned.columns)"
      ],
      "metadata": {
        "id": "XJKW4tXymU9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize questions into word arrays (split by space)\n",
        "d2_cleaned['tokenized_question'] = d2_cleaned['question'].apply(lambda x: x.split())\n",
        "\n",
        "# Find identical arrays (questions that have the same tokenized form)\n",
        "duplicate_questions = d2_cleaned[d2_cleaned.duplicated(subset=['tokenized_question'], keep=False)]\n",
        "\n",
        "# Display the duplicate questions (those with identical tokenized arrays)\n",
        "print(duplicate_questions[['question', 'tokenized_question']])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tVbUl293rotO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(duplicate_questions)"
      ],
      "metadata": {
        "id": "QgIrKNI7w-Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the questions into word arrays and convert to tuples for grouping\n",
        "d2_cleaned['tokenized_question'] = d2_cleaned['question'].apply(lambda x: tuple(x.split()))\n",
        "\n",
        "# Group by the tokenized_question column (now a tuple)\n",
        "grouped = d2_cleaned.groupby('tokenized_question')\n",
        "k=0\n",
        "# Iterate over groups with more than one entry and print the duplicate pairs\n",
        "for tokens, group in grouped:\n",
        "    if len(group) > 1:\n",
        "        k += len(group)\n",
        "        print(\"Matching tokenized question:\", tokens)\n",
        "        print(group[['question']])\n",
        "        print(\"-----\"*5)\n",
        "print(k)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oSptp22ks48u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2_filtered = d2_cleaned.drop_duplicates(subset=['tokenized_question']).reset_index(drop=True)\n",
        "\n",
        "duplicates = d2_filtered[d2_filtered.duplicated(subset=['tokenized_question'], keep=False)]\n",
        "len(duplicates)"
      ],
      "metadata": {
        "id": "z9SVYwPvxUfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d2_filtered.shape)\n",
        "d2_filtered.head()"
      ],
      "metadata": {
        "id": "hrX-wB8czbOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2_filtered.dropna(subset=['question', 'answer'], inplace=True)\n",
        "d2_filtered.shape"
      ],
      "metadata": {
        "id": "Nla-Kl0vnvyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = d2_filtered.copy()\n",
        "\n",
        "qa_df['question_length'] = qa_df['question'].astype(str).apply(lambda x: len(x.split()))\n",
        "qa_df['answer_length'] = qa_df['answer'].apply(lambda x: len(x.split()))\n",
        "\n",
        "qa_df.head()"
      ],
      "metadata": {
        "id": "eTPREhbnfQ8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_question_length = np.mean(qa_df['question_length'])\n",
        "avg_answer_length = np.mean(qa_df['answer_length'])\n",
        "\n",
        "print(f\"Average Question Length: {avg_question_length:.2f} words\")\n",
        "print(f\"Average Answer Length: {avg_answer_length:.2f} words\")"
      ],
      "metadata": {
        "id": "vZUUlWhvfRE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df['question'] = qa_df['question'].str.lower().str.strip()\n",
        "qa_df['answer'] = qa_df['answer'].str.lower().str.strip()\n",
        "\n",
        "qa_df.head()"
      ],
      "metadata": {
        "id": "zG5TeUWSm-k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = qa_df.drop(columns=['tokenized_question', 'question_length', 'answer_length'])\n",
        "qa_df.head()"
      ],
      "metadata": {
        "id": "RORhu_n1otP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = unicodedata.normalize(\"NFKC\", text)  # Normalize Unicode\n",
        "    text = text.lower().strip()  # Convert to lowercase\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "iFqyJRNIsxIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalize_text(qa_df['question'][0]))\n",
        "print(normalize_text(qa_df['answer'][0]))\n",
        "\n",
        "qa_df = qa_df.applymap(normalize_text)\n",
        "qa_df.head()"
      ],
      "metadata": {
        "id": "4uCPdaCwOtY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save as JSONL for LLM Fine-Tuning"
      ],
      "metadata": {
        "id": "PojwVHHco5cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save as JSONL (each line is a JSON object)\n",
        "with open(\"qa_dataset.jsonl\", \"w\") as f:\n",
        "    for _, row in qa_df.iterrows():\n",
        "        json.dump({\"question\": row[\"question\"], \"answer\": row[\"answer\"]}, f)\n",
        "        f.write(\"\\n\")  # Add newline to separate each JSON object\n"
      ],
      "metadata": {
        "id": "YwNWYxFHOrFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's count TF-IDF"
      ],
      "metadata": {
        "id": "9UxFOAN5qR2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "corpus = d2_filtered['question'].tolist()\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "\n",
        "# Get the mapping of tokens to feature indices\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "first_question_tfidf = pd.DataFrame(tfidf_matrix[0].T.todense(),\n",
        "                                    index=feature_names,\n",
        "                                    columns=[\"TF-IDF\"])\n",
        "print(first_question_tfidf.sort_values(\"TF-IDF\", ascending=False))\n"
      ],
      "metadata": {
        "id": "VazVsXc8ocGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download if not already\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# POS tag mapping function\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default to noun\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_counter = Counter()\n",
        "\n",
        "with open(\"qa_dataset.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        question = data[\"question\"].lower()\n",
        "        tokens = word_tokenize(question)\n",
        "        tagged = pos_tag(tokens)\n",
        "\n",
        "        lemmatized = [\n",
        "            lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "            for word, tag in tagged if word.isalpha() and word not in stop_words\n",
        "        ]\n",
        "\n",
        "        word_counter.update(lemmatized)\n",
        "\n",
        "print(word_counter.most_common(20))\n"
      ],
      "metadata": {
        "id": "JgXMjGp15hbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "qa_word_counter_df = pd.DataFrame(word_counter.most_common(20), columns=['Word', 'Frequency'])\n",
        "qa_word_counter_df"
      ],
      "metadata": {
        "id": "811CnzzzuAmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# Create a dictionary with words as keys and their TF-IDF scores as values\n",
        "words_tfidf = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).A1))\n",
        "\n",
        "# Create the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words_tfidf)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YM_rTN-b5mmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert the TF-IDF matrix to a dense format and create a DataFrame\n",
        "# tfidf_df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
        "\n",
        "# # Now, you can get a DataFrame of every token's TF-IDF score\n",
        "# # For example, to see the tokens and their corresponding TF-IDF values for the entire corpus:\n",
        "# tokens_tfidf = tfidf_df.stack().reset_index(name='TF-IDF')\n",
        "# tokens_tfidf.columns = ['Document', 'Token', 'TF-IDF']\n",
        "# tokens_tfidf.to_csv('tokens_tfidf.csv', index=False)"
      ],
      "metadata": {
        "id": "2eBVp-8B3YQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "metadata": {
        "id": "Fk25LPNY5KFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset #3"
      ],
      "metadata": {
        "id": "AWac_MXtT8Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "d3_path = \"/content/drive/MyDrive/datasets/advs7235-sup-0003-suppmat.csv\"\n",
        "\n",
        "d3 = pd.read_csv(d3_path)\n",
        "d3.head()"
      ],
      "metadata": {
        "id": "yfIUs9yTTc8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d3.shape)\n",
        "print(d3.columns)"
      ],
      "metadata": {
        "id": "QqR2asv8uhz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d3.describe()"
      ],
      "metadata": {
        "id": "-vsaPRs-Tspn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d3 = d3.drop(columns=['Citation'])\n",
        "d3.head(2)"
      ],
      "metadata": {
        "id": "qzVbzDOcei7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d3_duplicates = d3[d3.duplicated(subset=['Question'], keep=False)]\n",
        "d3_duplicates.count()"
      ],
      "metadata": {
        "id": "6fSbufyZTxD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d3.shape"
      ],
      "metadata": {
        "id": "Imwv9x5viWRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_df = d3.copy()\n",
        "\n",
        "mc_df['Question_length'] = mc_df['Question'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "mc_df.head()"
      ],
      "metadata": {
        "id": "zPDIn4uQgNm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_avg_length = np.mean(mc_df['Question_length'])\n",
        "\n",
        "print(f\"Average Question Length: {question_avg_length:.2f} words\")"
      ],
      "metadata": {
        "id": "T5YAgiliihlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "category_counts = mc_df['Category'].value_counts().astype(int)\n",
        "\n",
        "# Get the labels corresponding to the counts\n",
        "labels = category_counts.index.tolist()  # Use the index for labels\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.pie(category_counts.values, labels=labels)  # Pass values and labels separately\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJOpaEmEirZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_df.dropna(subset=['Question', 'Answer'], inplace=True)\n",
        "mc_df = mc_df.drop(columns='Question_length')\n",
        "mc_df.head()"
      ],
      "metadata": {
        "id": "wVkjwlVfjI36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalize text"
      ],
      "metadata": {
        "id": "-JuhmLhvv8mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalize_text(mc_df['Question'][0]))\n",
        "print(normalize_text(mc_df['Answer'][0]))\n",
        "\n",
        "mc_df = mc_df.applymap(normalize_text)\n",
        "mc_df.head()"
      ],
      "metadata": {
        "id": "kB-SPTvgvyMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "corpus = mc_df['Question'].tolist()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "first_question_tfidf = pd.DataFrame(tfidf_matrix[0].T.todense(),\n",
        "                                    index=feature_names,\n",
        "                                    columns=[\"TF-IDF\"])\n",
        "print(first_question_tfidf.sort_values(\"TF-IDF\", ascending=False))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pRhQD4URlWXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a dictionary with words as keys and their TF-IDF scores as values\n",
        "words_tfidf = dict(zip(vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).A1))\n",
        "\n",
        "# Create the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(words_tfidf)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n_ROuhLZ0l67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_df = mc_df.drop(columns='Question_length')\n",
        "mc_df.head()"
      ],
      "metadata": {
        "id": "mdCe6SMIWuku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Save as JSONL (each line is a JSON object)\n",
        "with open(\"mc_dataset.jsonl\", \"w\") as f:\n",
        "    for _, row in mc_df.iterrows():\n",
        "        json.dump({\"question\": row[\"Question\"], \"answer\": row[\"Answer\"], \"category\": row[\"Category\"]}, f)\n",
        "        f.write(\"\\n\")  # Add newline to separate each JSON object\n"
      ],
      "metadata": {
        "id": "ryiQCFMnU36w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download if not already\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# POS tag mapping function\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default to noun\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add('a')\n",
        "stop_words.add('b')\n",
        "stop_words.add('c')\n",
        "stop_words.add('d')\n",
        "\n",
        "mc_word_counter = Counter()\n",
        "\n",
        "with open(\"mc_dataset.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        question = data[\"question\"].lower()\n",
        "        tokens = word_tokenize(question)\n",
        "        tagged = pos_tag(tokens)\n",
        "\n",
        "        lemmatized = [\n",
        "            lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "            for word, tag in tagged if word.isalpha() and word not in stop_words\n",
        "        ]\n",
        "\n",
        "        mc_word_counter.update(lemmatized)\n",
        "\n",
        "print(mc_word_counter.most_common(20))\n"
      ],
      "metadata": {
        "id": "kJ9BfYrAz-m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_word_counter_df = pd.DataFrame(mc_word_counter.most_common(20), columns=['Word', 'Frequency'])\n",
        "mc_word_counter_df"
      ],
      "metadata": {
        "id": "HUIEpKVJwsdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F2IiMzGywu2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}