{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzFCf1iUhMfP"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "KTmX5dbyRAhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwvC-b3-3KFo"
      },
      "source": [
        "### Tokens & APIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjIXQ3DK3U-i"
      },
      "outputs": [],
      "source": [
        "huggingface_token = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "some_token = \"\""
      ],
      "metadata": {
        "id": "-T9ENlCWEaDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vQNY_X3oRKi"
      },
      "outputs": [],
      "source": [
        "wandb_token = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv_q71ahe3lH"
      },
      "outputs": [],
      "source": [
        "openai_key = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd7byJEL3ZOh"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0I-P5ul8YDN3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QBzs3BNqi5GN"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y bitsandbytes\n",
        "!pip install -U bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vtiqompvPKC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JiyUjbMd4tdo"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCxOBWLkjZ77"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes as bnb\n",
        "print(bnb.__version__)  # Should print a valid version number\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enter the huggingface token when prompted"
      ],
      "metadata": {
        "id": "OdhrOma0zefS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iTlnrUpegjIW"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lefrGxoi29vJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LrHfXYz3ds2"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JmWrVAt3I-Z"
      },
      "source": [
        "## Model Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7B4JX01e_ls"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# Configuratiion of 4-bit quantization using bitsandbytes\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Enable 4-bit quantization\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Use NormalFloat4 for better precision\n",
        "    bnb_4bit_use_double_quant=True,  # Enable double quantization\n",
        "    bnb_4bit_compute_dtype=torch.float16  # Compute in int8\n",
        ")\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Verify model is in 4-bit mode\n",
        "print(model.hf_device_map)  # Show which devices the model is loaded on\n",
        "print(model.dtype)  # Should print torch.float16 (for computation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "\n",
        "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# fine_tuned_checkpoint = \"/content/drive/MyDrive/datasets/llama_checkpoint_3000\"\n",
        "# output_path = \"/content/drive/MyDrive/datasets/llama\"\n",
        "\n"
      ],
      "metadata": {
        "id": "5AH2BwHDI8Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u1c5RVtsB5e"
      },
      "source": [
        "## QA Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTepLRxT25QE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#Loads a JSONL file where each line is {\"question\": ..., \"answer\": ...}\n",
        "dataset = load_dataset(\"json\", data_files=\"qa_dataset.jsonl\", split=\"train\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UzMKkH6l9EZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token  # for LLaMA2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL08J6GH9EgG"
      },
      "outputs": [],
      "source": [
        "def tokenize_qa(example):\n",
        "    prompt = f\"Q: {example['question']}\\nA: {example['answer']}\"\n",
        "    tokens = tokenizer(prompt,\n",
        "                       truncation=True,\n",
        "                       max_length=512,\n",
        "                       padding=\"max_length\")\n",
        "\n",
        "    # Labels = input_ids for causal LM\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jehyGGMZokqs"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(tokenize_qa, batched=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8V2CKPzr_uz"
      },
      "source": [
        "## Lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3sbAG2vsOFz"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRRNDGd8knS_"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig( #4. Experimental Section\n",
        "    r=96,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QXQgpHnUrjZ"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsCaJ8wt9UvG"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama2_finetune\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=0.0002,\n",
        "    adam_epsilon=1e-8,\n",
        "    max_steps=3000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"wandb\",  # logs to WandB\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    # load_best_model_at_end=True,\n",
        "    eval_steps=500,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    # evaluation_strategy=\"no\",\n",
        "    max_grad_norm=0.3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN5RxZAK-I2s"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# train.train() # load from checkpoint after 500 steps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y4kpE5Knpl3"
      },
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXuTlrPjSfXh"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set env & install libraries"
      ],
      "metadata": {
        "id": "s9BgEovS0vAY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdXpcz9D8wq2"
      },
      "outputs": [],
      "source": [
        "fine_tuned_checkpoint = \"your_checkpoint_dir\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "WN7CXLdCUkKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ult4N0DJgB1f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_key\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model_name = \"BAAI/bge-large-en\""
      ],
      "metadata": {
        "id": "04PFhZWbPAb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XzuY-8bzjm1E"
      },
      "outputs": [],
      "source": [
        "!pip install -U llama-index llama-index-core llama-index-vector-stores-chroma llama-index-llms-huggingface\n",
        "\n",
        "# LangChain and Chroma for vector store + embeddings\n",
        "!pip install -U langchain chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1yz7sZrbTBVO"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iSdelQA9UWO7"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xxuOtyO3U-0c"
      },
      "outputs": [],
      "source": [
        "# Install HuggingFace embeddings + LLM support\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-llms-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAQGhbi0Sw1R"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkjuCIONb0_F"
      },
      "source": [
        "### Get the model and tokenizer from the checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmvqISWuS_O6"
      },
      "outputs": [],
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    token=huggingface_token\n",
        ")\n",
        "\n",
        "model_from_checkpoint = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    fine_tuned_checkpoint,\n",
        "    token=huggingface_token\n",
        ")\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=huggingface_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTntnQlTcVvs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WutkptpUYuQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=embedding_model_name) # needs to set internal embedding model to ours to not cause confliction to default of Llamaindex\n",
        "# Settings.num_output = 312\n",
        "# Settings.context_window = 3900"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the document to compile the Chroma vector db"
      ],
      "metadata": {
        "id": "gwPJnbTX3tfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(input_files=[\"document.json_path\"]).load_data()"
      ],
      "metadata": {
        "id": "GEBrhRRC3ss1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWIhkQxub6I3"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "chunks = []\n",
        "for doc in documents:\n",
        "    for chunk in splitter.split_text(doc.text):\n",
        "        chunks.append(Document(page_content=chunk, metadata=doc.metadata))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXGNw_z-iJGf"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"chroma_db\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJxgihulbzK"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores import ChromaVectorStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index.indices.vector_store import VectorStoreIndex\n",
        "\n",
        "chroma_vectorstore = ChromaVectorStore(chroma_collection=vectorstore._collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=chroma_vectorstore)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents=chunks, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB6tiEBPl4_T"
      },
      "source": [
        "#### Save the index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvul44Fjl7Nv"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=chunks,\n",
        "    storage_context=storage_context,\n",
        "    persist_dir=\"index_storage\"\n",
        ")\n",
        "index.storage_context.persist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MNNi5g7d2EI"
      },
      "source": [
        "### Creating a Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGBzuH-xcvA_"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6F9B88Cd-qT"
      },
      "outputs": [],
      "source": [
        "# response = query_engine.query(\"What are latest discoveries in bio-related field?\")\n",
        "# print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcfuLG9hiPBu"
      },
      "outputs": [],
      "source": [
        "# q = \"What methods can be used to assess the adsorption of enzymes onto graphite electrodes, and how can the activities of immobilized laccase be evaluated?\"\n",
        "\n",
        "\n",
        "# a = query_engine.query(q)\n",
        "# print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gii9JXKhiLE5"
      },
      "source": [
        "### Test the performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOlVFbP2zA2m"
      },
      "source": [
        "Need to re-evaluate the result csv file manually, to confirm the answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxijqT9f0EaG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "mc_dataset_path = \"define_path_to_mc_dataset.jsonl\"\n",
        "\n",
        "with open(mc_dataset_path, \"r\") as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Prompt engineer (depending on the prompt, model's behaviour to answer correctly differ.)\n",
        "def format_prompt(q):\n",
        "    return f\"\"\"You will be given a multiple-choice question. Respond with only the letter corresponding to the correct answer (A, B, C). Do not include explanations or restate the question.\n",
        "            Question: {q}\n",
        "            Answer:\"\"\"\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, use_rag=False, retriever=None, temperature=0.1):\n",
        "\n",
        "    model.eval()\n",
        "    results = []\n",
        "\n",
        "    for d in dataset:\n",
        "        question = d[\"question\"]\n",
        "        gt_answer = d[\"answer\"].strip().upper()\n",
        "        category = d.get('category', 'Unknown')\n",
        "\n",
        "        # If RAG, retrieve context\n",
        "        if use_rag and retriever is not None:\n",
        "            context = retriever.retrieve(question)\n",
        "            if isinstance(context, list):  # If retriever returns list of texts\n",
        "                context = \"\\n\".join(context)\n",
        "            prompt = f\"\"\"Context:\\n\\n{context}\\n\\nQuestion:{question}\\n\\Answer:\"\"\"\n",
        "        else:\n",
        "            prompt = format_prompt(question)\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        response = decoded.strip().split()[-1][0].upper()\n",
        "\n",
        "\n",
        "        predicted = None\n",
        "        for choice in ['A', 'B', 'C']:\n",
        "            if choice in response:\n",
        "                predicted = choice\n",
        "                break\n",
        "        if predicted is None:\n",
        "            predicted = \"Unknown\"\n",
        "\n",
        "        print(f\"GT: {gt_answer}, Predicted: {predicted}, Full Response: {decoded}\")\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"full_response\": decoded,\n",
        "            \"predicted\": predicted,\n",
        "            \"ground_truth\": gt_answer,\n",
        "            \"category\": category,\n",
        "        })\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Base model eval"
      ],
      "metadata": {
        "id": "FlpfkQ-ZEfO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPUNH0ywlsBG"
      },
      "outputs": [],
      "source": [
        "base_eval = evaluate_model(base_model, tokenizer, documents, use_rag=False)\n",
        "base_eval.to_save('your_base_model_eval.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BioLLM eval"
      ],
      "metadata": {
        "id": "5RRSvW-gEieo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnrbwqkG1FXQ"
      },
      "outputs": [],
      "source": [
        "bio_eval = evaluate_model(model_from_checkpoint, tokenizer, documents, use_rag=False)\n",
        "bio_eval.to_save('your_bio_model_eval.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BioLLM+RAG"
      ],
      "metadata": {
        "id": "xg-Ko06jFHZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BioLLM:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = model.device\n",
        "\n",
        "    def complete(self, prompt, max_tokens=100):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return decoded.strip()\n",
        "\n",
        "biollm = BioLLM(model=model_from_checkpoint, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "T-bt0_SpQBJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9reJ-QjizZy4"
      },
      "outputs": [],
      "source": [
        "rag_eval = evaluate_model(biollm, tokenizer, documents, use_rag=True, retriever=query_engine)\n",
        "rag_eval.to_save('your_rag_eval.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface"
      ],
      "metadata": {
        "id": "YIialJWuHXxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers llama-index sentence-transformers\n",
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g9wsS1HZRcQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AANTjQrV3eH4"
      },
      "source": [
        "### Load from saved index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH6TPW0ezbpu"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# econnect to ChromaDB\n",
        "\n",
        "\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=\"chromadb_dir\",\n",
        "    embedding_function=embedding_model,\n",
        ")\n",
        "\n",
        "chroma_collection = vectorstore._collection\n",
        "chroma_vectorstore = ChromaVectorStore(chroma_collection=chroma_collection)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define device to gpu, if not cpu"
      ],
      "metadata": {
        "id": "oIsWu1FiHauk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model_from_checkpoint.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WMPoD7XDTIX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Chroma vector store\n",
        "embedding_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n",
        "chroma_vectorstore = Chroma(\n",
        "    persist_directory='chroma_db_dir',\n",
        "    embedding_function=embedding_model,\n",
        ")\n",
        "chroma_collection = chroma_vectorstore._collection\n",
        "chroma_vectorstore = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Load LlamaIndex from storage\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    persist_dir='index_dir',\n",
        "    vector_store=chroma_vectorstore\n",
        ")\n",
        "index = load_index_from_storage(storage_context)\n",
        "\n",
        "# Set up retriever and reranker\n",
        "reranker = SentenceTransformerRerank(\n",
        "    top_n=2, # define depending on your preference\n",
        "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
        "    keep_retrieval_score=False\n",
        ")\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=5, # more the slower, the more accurate\n",
        "    node_postprocessors=[reranker]\n",
        ")\n"
      ],
      "metadata": {
        "id": "uhoVvwadQdbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "def chat_fn(message):\n",
        "    try:\n",
        "        # Retrieve relevant context\n",
        "        retrieved_nodes = retriever.retrieve(message)\n",
        "        context = \"\\n\\n\".join([node.text for node in retrieved_nodes])\n",
        "\n",
        "        # Build prompt\n",
        "        prompt = f\"\"\"You are a biomedical expert. Use the given context to answer the question in a concise and clear manner.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {message}\n",
        "Answer:\"\"\"\n",
        "\n",
        "        raw_response = bio_llm.complete(prompt, max_tokens=256).strip()\n",
        "\n",
        "        # if \"answer:\" in raw_response.lower():\n",
        "        #     cleaned = raw_response.lower().split(\"answer:\")[-1].strip()\n",
        "        # else:\n",
        "        #     cleaned = raw_response\n",
        "        raw_lower = raw_response.lower()\n",
        "        if \"answer:\" in raw_lower:\n",
        "            idx = raw_lower.find(\"answer:\")  # Find where 'answer:' starts (case-insensitive)\n",
        "            cleaned = raw_response[idx + len(\"answer:\"):].strip()  # Extract substring from original text\n",
        "        else:\n",
        "            cleaned = raw_response.strip()\n",
        "\n",
        "\n",
        "        # if cleaned:\n",
        "        #     cleaned = cleaned[0].upper() + cleaned[1:]\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    except torch.cuda.OutOfMemoryError:\n",
        "        torch.cuda.empty_cache()\n",
        "        return \"⚠️ CUDA out of memory. Try a shorter query or reduce context length.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Error: {str(e)}\"\n",
        "\n",
        "    finally:\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "cmlnu0DEQoVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chat_interface():\n",
        "    def respond(message, history):\n",
        "        time.sleep(0.05)\n",
        "        response = chat_fn(message)\n",
        "        history.append((message, response))\n",
        "        return \"\", history\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"# BioLLM + RAG Chat\")\n",
        "        gr.Markdown(\"Ask me biomedical questions! I use BioLLM + literature RAG for answers.\")\n",
        "\n",
        "        chatbot = gr.Chatbot()\n",
        "        msg = gr.Textbox(placeholder=\"Type your question and press Enter...\")\n",
        "\n",
        "        clear = gr.Button(\"Clear\")\n",
        "\n",
        "        state = gr.State([])\n",
        "\n",
        "        msg.submit(respond, [msg, state], [msg, chatbot])\n",
        "        clear.click(lambda: ([], \"\"), None, [chatbot, msg])\n",
        "\n",
        "    return demo\n"
      ],
      "metadata": {
        "id": "DCHOazUlQqo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chat_app = build_chat_interface()\n",
        "    chat_app.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "aknk8SzlQtdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}